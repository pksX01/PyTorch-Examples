{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network_basic_example_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pksX01/PyTorch-Examples/blob/master/neural_network_basic_example_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lt1dncc9iYb",
        "colab_type": "code",
        "outputId": "cdf71206-9a34-4336-9743-0bb98cfb4169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLiRDE4w_JmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRpEXs4U_wUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float)\n",
        "y = torch.tensor(([92], [100], [89]), dtype=torch.float) \n",
        "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6cmGDgEBQtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_max, _ = torch.max(X, 0)\n",
        "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
        "\n",
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
        "y = y / 100  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K6Teu10CRn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "       \n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "        \n",
        "        # weights\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) \n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.z = torch.matmul(X, self.W1) \n",
        "        self.z2 = self.sigmoid(self.z) \n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        o = self.sigmoid(self.z3)\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o # error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        # forward + backward pass for training\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "        \n",
        "    def saveWeights(self, model):\n",
        "        # we will use the PyTorch internal storage functions\n",
        "        torch.save(model, \"NN\")\n",
        "        # you can reload model with all the weights and so forth with:\n",
        "        # torch.load(\"NN\")\n",
        "        \n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoDzd1JWTEMl",
        "colab_type": "code",
        "outputId": "03557970-0df5-4683-9d80-816fd0d6c55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17156
        }
      },
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(1000):  # trains the NN 1,000 times\n",
        "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "NN.saveWeights(NN)\n",
        "NN.predict()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.2714870274066925\n",
            "#1 Loss: 0.24636049568653107\n",
            "#2 Loss: 0.22435711324214935\n",
            "#3 Loss: 0.20477800071239471\n",
            "#4 Loss: 0.18709571659564972\n",
            "#5 Loss: 0.17092092335224152\n",
            "#6 Loss: 0.15597249567508698\n",
            "#7 Loss: 0.14205361902713776\n",
            "#8 Loss: 0.12903347611427307\n",
            "#9 Loss: 0.11683255434036255\n",
            "#10 Loss: 0.105410136282444\n",
            "#11 Loss: 0.09475304931402206\n",
            "#12 Loss: 0.08486463874578476\n",
            "#13 Loss: 0.0757550597190857\n",
            "#14 Loss: 0.06743218004703522\n",
            "#15 Loss: 0.05989484116435051\n",
            "#16 Loss: 0.05312873050570488\n",
            "#17 Loss: 0.047104984521865845\n",
            "#18 Loss: 0.04178149625658989\n",
            "#19 Loss: 0.03710578754544258\n",
            "#20 Loss: 0.033018868416547775\n",
            "#21 Loss: 0.02945900522172451\n",
            "#22 Loss: 0.02636503428220749\n",
            "#23 Loss: 0.023678714409470558\n",
            "#24 Loss: 0.021346407011151314\n",
            "#25 Loss: 0.01931978017091751\n",
            "#26 Loss: 0.017556127160787582\n",
            "#27 Loss: 0.016018236055970192\n",
            "#28 Loss: 0.01467397902160883\n",
            "#29 Loss: 0.013495790772140026\n",
            "#30 Loss: 0.0124601935967803\n",
            "#31 Loss: 0.011547181755304337\n",
            "#32 Loss: 0.010739793069660664\n",
            "#33 Loss: 0.010023622773587704\n",
            "#34 Loss: 0.00938641931861639\n",
            "#35 Loss: 0.008817764930427074\n",
            "#36 Loss: 0.00830881018191576\n",
            "#37 Loss: 0.007851961068809032\n",
            "#38 Loss: 0.0074407621286809444\n",
            "#39 Loss: 0.00706966407597065\n",
            "#40 Loss: 0.00673386687412858\n",
            "#41 Loss: 0.006429269909858704\n",
            "#42 Loss: 0.006152316462248564\n",
            "#43 Loss: 0.005899913143366575\n",
            "#44 Loss: 0.005669370759278536\n",
            "#45 Loss: 0.005458344239741564\n",
            "#46 Loss: 0.0052647944539785385\n",
            "#47 Loss: 0.005086924415081739\n",
            "#48 Loss: 0.0049231573939323425\n",
            "#49 Loss: 0.0047721038572490215\n",
            "#50 Loss: 0.004632533993571997\n",
            "#51 Loss: 0.004503361415117979\n",
            "#52 Loss: 0.0043836175464093685\n",
            "#53 Loss: 0.004272442776709795\n",
            "#54 Loss: 0.004169072490185499\n",
            "#55 Loss: 0.00407281843945384\n",
            "#56 Loss: 0.003983078990131617\n",
            "#57 Loss: 0.0038992834743112326\n",
            "#58 Loss: 0.0038209501653909683\n",
            "#59 Loss: 0.003747634356841445\n",
            "#60 Loss: 0.003678922774270177\n",
            "#61 Loss: 0.0036144666373729706\n",
            "#62 Loss: 0.003553914837539196\n",
            "#63 Loss: 0.0034969907719641924\n",
            "#64 Loss: 0.0034434066619724035\n",
            "#65 Loss: 0.0033929182682186365\n",
            "#66 Loss: 0.0033453076612204313\n",
            "#67 Loss: 0.00330035830847919\n",
            "#68 Loss: 0.0032578769605606794\n",
            "#69 Loss: 0.0032177064567804337\n",
            "#70 Loss: 0.0031796821858733892\n",
            "#71 Loss: 0.003143657697364688\n",
            "#72 Loss: 0.003109496086835861\n",
            "#73 Loss: 0.003077080240473151\n",
            "#74 Loss: 0.0030462974682450294\n",
            "#75 Loss: 0.0030170362442731857\n",
            "#76 Loss: 0.0029891980811953545\n",
            "#77 Loss: 0.0029627082403749228\n",
            "#78 Loss: 0.0029374726582318544\n",
            "#79 Loss: 0.0029134126380085945\n",
            "#80 Loss: 0.002890464151278138\n",
            "#81 Loss: 0.0028685638681054115\n",
            "#82 Loss: 0.0028476340230554342\n",
            "#83 Loss: 0.0028276366647332907\n",
            "#84 Loss: 0.0028085075318813324\n",
            "#85 Loss: 0.0027901949360966682\n",
            "#86 Loss: 0.002772666746750474\n",
            "#87 Loss: 0.002755860798060894\n",
            "#88 Loss: 0.002739759860560298\n",
            "#89 Loss: 0.0027242982760071754\n",
            "#90 Loss: 0.0027094641700387\n",
            "#91 Loss: 0.002695213770493865\n",
            "#92 Loss: 0.0026815179735422134\n",
            "#93 Loss: 0.0026683493051677942\n",
            "#94 Loss: 0.0026556795928627253\n",
            "#95 Loss: 0.0026434820611029863\n",
            "#96 Loss: 0.0026317329611629248\n",
            "#97 Loss: 0.0026204045861959457\n",
            "#98 Loss: 0.0026094878558069468\n",
            "#99 Loss: 0.002598955063149333\n",
            "#100 Loss: 0.0025887873489409685\n",
            "#101 Loss: 0.0025789684150367975\n",
            "#102 Loss: 0.0025694766081869602\n",
            "#103 Loss: 0.0025603047106415033\n",
            "#104 Loss: 0.0025514313019812107\n",
            "#105 Loss: 0.002542845206335187\n",
            "#106 Loss: 0.0025345354806631804\n",
            "#107 Loss: 0.002526475815102458\n",
            "#108 Loss: 0.002518673427402973\n",
            "#109 Loss: 0.0025111103896051645\n",
            "#110 Loss: 0.002503767143934965\n",
            "#111 Loss: 0.0024966413620859385\n",
            "#112 Loss: 0.002489723963662982\n",
            "#113 Loss: 0.002483000745996833\n",
            "#114 Loss: 0.002476470312103629\n",
            "#115 Loss: 0.0024701172951608896\n",
            "#116 Loss: 0.00246393377892673\n",
            "#117 Loss: 0.002457920229062438\n",
            "#118 Loss: 0.002452067332342267\n",
            "#119 Loss: 0.002446359023451805\n",
            "#120 Loss: 0.0024408011231571436\n",
            "#121 Loss: 0.002435375703498721\n",
            "#122 Loss: 0.002430090680718422\n",
            "#123 Loss: 0.0024249304551631212\n",
            "#124 Loss: 0.0024198894388973713\n",
            "#125 Loss: 0.002414969028905034\n",
            "#126 Loss: 0.002410164335742593\n",
            "#127 Loss: 0.002405460923910141\n",
            "#128 Loss: 0.002400860656052828\n",
            "#129 Loss: 0.002396358409896493\n",
            "#130 Loss: 0.0023919623345136642\n",
            "#131 Loss: 0.002387650078162551\n",
            "#132 Loss: 0.0023834218736737967\n",
            "#133 Loss: 0.0023792902939021587\n",
            "#134 Loss: 0.002375231124460697\n",
            "#135 Loss: 0.002371253212913871\n",
            "#136 Loss: 0.002367339562624693\n",
            "#137 Loss: 0.00236351415514946\n",
            "#138 Loss: 0.002359747886657715\n",
            "#139 Loss: 0.0023560530971735716\n",
            "#140 Loss: 0.0023524202406406403\n",
            "#141 Loss: 0.002348855836316943\n",
            "#142 Loss: 0.002345347311347723\n",
            "#143 Loss: 0.0023418881464749575\n",
            "#144 Loss: 0.002338490216061473\n",
            "#145 Loss: 0.0023351481650024652\n",
            "#146 Loss: 0.0023318545427173376\n",
            "#147 Loss: 0.0023286070208996534\n",
            "#148 Loss: 0.002325414912775159\n",
            "#149 Loss: 0.0023222591262310743\n",
            "#150 Loss: 0.0023191554937511683\n",
            "#151 Loss: 0.002316088182851672\n",
            "#152 Loss: 0.0023130683694034815\n",
            "#153 Loss: 0.002310082083567977\n",
            "#154 Loss: 0.0023071353789418936\n",
            "#155 Loss: 0.0023042222019284964\n",
            "#156 Loss: 0.0023013486061245203\n",
            "#157 Loss: 0.0022985145915299654\n",
            "#158 Loss: 0.0022957075852900743\n",
            "#159 Loss: 0.0022929320111870766\n",
            "#160 Loss: 0.002290185308083892\n",
            "#161 Loss: 0.0022874725982546806\n",
            "#162 Loss: 0.0022847834043204784\n",
            "#163 Loss: 0.0022821256425231695\n",
            "#164 Loss: 0.002279492560774088\n",
            "#165 Loss: 0.0022768874187022448\n",
            "#166 Loss: 0.0022743071895092726\n",
            "#167 Loss: 0.0022717462852597237\n",
            "#168 Loss: 0.002269214252009988\n",
            "#169 Loss: 0.002266702940687537\n",
            "#170 Loss: 0.002264211419969797\n",
            "#171 Loss: 0.002261742018163204\n",
            "#172 Loss: 0.0022592905443161726\n",
            "#173 Loss: 0.0022568649146705866\n",
            "#174 Loss: 0.002254455117508769\n",
            "#175 Loss: 0.002252058358862996\n",
            "#176 Loss: 0.002249686745926738\n",
            "#177 Loss: 0.0022473251447081566\n",
            "#178 Loss: 0.002244985429570079\n",
            "#179 Loss: 0.002242659218609333\n",
            "#180 Loss: 0.0022403495386242867\n",
            "#181 Loss: 0.0022380556911230087\n",
            "#182 Loss: 0.0022357709240168333\n",
            "#183 Loss: 0.002233505016192794\n",
            "#184 Loss: 0.002231254708021879\n",
            "#185 Loss: 0.0022290118504315615\n",
            "#186 Loss: 0.0022267820313572884\n",
            "#187 Loss: 0.0022245675791054964\n",
            "#188 Loss: 0.002222365001216531\n",
            "#189 Loss: 0.0022201742976903915\n",
            "#190 Loss: 0.002217991277575493\n",
            "#191 Loss: 0.0022158219944685698\n",
            "#192 Loss: 0.0022136601619422436\n",
            "#193 Loss: 0.0022115099709481\n",
            "#194 Loss: 0.002209368860349059\n",
            "#195 Loss: 0.0022072389256209135\n",
            "#196 Loss: 0.002205116907134652\n",
            "#197 Loss: 0.0022030023392289877\n",
            "#198 Loss: 0.002200903370976448\n",
            "#199 Loss: 0.0021988016087561846\n",
            "#200 Loss: 0.002196712652221322\n",
            "#201 Loss: 0.0021946330089122057\n",
            "#202 Loss: 0.002192561747506261\n",
            "#203 Loss: 0.002190493745729327\n",
            "#204 Loss: 0.002188438316807151\n",
            "#205 Loss: 0.0021863868460059166\n",
            "#206 Loss: 0.002184339100494981\n",
            "#207 Loss: 0.002182302065193653\n",
            "#208 Loss: 0.002180268755182624\n",
            "#209 Loss: 0.0021782428957521915\n",
            "#210 Loss: 0.002176225185394287\n",
            "#211 Loss: 0.0021742116659879684\n",
            "#212 Loss: 0.0021722011733800173\n",
            "#213 Loss: 0.0021701997611671686\n",
            "#214 Loss: 0.002168196951970458\n",
            "#215 Loss: 0.0021662067156285048\n",
            "#216 Loss: 0.0021642185747623444\n",
            "#217 Loss: 0.0021622360218316317\n",
            "#218 Loss: 0.0021602583583444357\n",
            "#219 Loss: 0.002158285351470113\n",
            "#220 Loss: 0.002156314207240939\n",
            "#221 Loss: 0.0021543484181165695\n",
            "#222 Loss: 0.002152387285605073\n",
            "#223 Loss: 0.002150424988940358\n",
            "#224 Loss: 0.00214847712777555\n",
            "#225 Loss: 0.0021465232130140066\n",
            "#226 Loss: 0.002144579077139497\n",
            "#227 Loss: 0.0021426330786198378\n",
            "#228 Loss: 0.0021406987216323614\n",
            "#229 Loss: 0.0021387601736932993\n",
            "#230 Loss: 0.0021368288435041904\n",
            "#231 Loss: 0.0021348989102989435\n",
            "#232 Loss: 0.002132974099367857\n",
            "#233 Loss: 0.0021310471929609776\n",
            "#234 Loss: 0.002129131928086281\n",
            "#235 Loss: 0.002127215266227722\n",
            "#236 Loss: 0.0021252997685223818\n",
            "#237 Loss: 0.0021233821753412485\n",
            "#238 Loss: 0.0021214757580310106\n",
            "#239 Loss: 0.0021195707377046347\n",
            "#240 Loss: 0.0021176645532250404\n",
            "#241 Loss: 0.002115763956680894\n",
            "#242 Loss: 0.0021138659212738276\n",
            "#243 Loss: 0.0021119711454957724\n",
            "#244 Loss: 0.0021100721787661314\n",
            "#245 Loss: 0.002108181593939662\n",
            "#246 Loss: 0.002106288680806756\n",
            "#247 Loss: 0.002104406477883458\n",
            "#248 Loss: 0.002102517755702138\n",
            "#249 Loss: 0.0021006332244724035\n",
            "#250 Loss: 0.002098751487210393\n",
            "#251 Loss: 0.002096869284287095\n",
            "#252 Loss: 0.0020949942991137505\n",
            "#253 Loss: 0.00209311512298882\n",
            "#254 Loss: 0.002091241767629981\n",
            "#255 Loss: 0.002089370507746935\n",
            "#256 Loss: 0.0020874962210655212\n",
            "#257 Loss: 0.002085624262690544\n",
            "#258 Loss: 0.002083763014525175\n",
            "#259 Loss: 0.002081893151625991\n",
            "#260 Loss: 0.0020800286438316107\n",
            "#261 Loss: 0.0020781662315130234\n",
            "#262 Loss: 0.0020763035863637924\n",
            "#263 Loss: 0.0020744442008435726\n",
            "#264 Loss: 0.002072584815323353\n",
            "#265 Loss: 0.0020707256626337767\n",
            "#266 Loss: 0.002068871632218361\n",
            "#267 Loss: 0.0020670127123594284\n",
            "#268 Loss: 0.0020651628728955984\n",
            "#269 Loss: 0.0020633090753108263\n",
            "#270 Loss: 0.002061458770185709\n",
            "#271 Loss: 0.0020596140529960394\n",
            "#272 Loss: 0.002057765144854784\n",
            "#273 Loss: 0.002055915305390954\n",
            "#274 Loss: 0.002054070821031928\n",
            "#275 Loss: 0.002052230527624488\n",
            "#276 Loss: 0.0020503841806203127\n",
            "#277 Loss: 0.0020485427230596542\n",
            "#278 Loss: 0.00204669963568449\n",
            "#279 Loss: 0.002044862834736705\n",
            "#280 Loss: 0.0020430234726518393\n",
            "#281 Loss: 0.0020411855075508356\n",
            "#282 Loss: 0.002039353596046567\n",
            "#283 Loss: 0.0020375153981149197\n",
            "#284 Loss: 0.002035679994150996\n",
            "#285 Loss: 0.002033850410953164\n",
            "#286 Loss: 0.002032018033787608\n",
            "#287 Loss: 0.0020301879849284887\n",
            "#288 Loss: 0.0020283551421016455\n",
            "#289 Loss: 0.002026525093242526\n",
            "#290 Loss: 0.0020246959757059813\n",
            "#291 Loss: 0.0020228743087500334\n",
            "#292 Loss: 0.002021047053858638\n",
            "#293 Loss: 0.0020192256197333336\n",
            "#294 Loss: 0.0020173953380435705\n",
            "#295 Loss: 0.0020155725069344044\n",
            "#296 Loss: 0.002013752469792962\n",
            "#297 Loss: 0.002011932199820876\n",
            "#298 Loss: 0.0020101135596632957\n",
            "#299 Loss: 0.002008291194215417\n",
            "#300 Loss: 0.002006473019719124\n",
            "#301 Loss: 0.0020046555437147617\n",
            "#302 Loss: 0.00200284062884748\n",
            "#303 Loss: 0.0020010273437947035\n",
            "#304 Loss: 0.001999212894588709\n",
            "#305 Loss: 0.001997399376705289\n",
            "#306 Loss: 0.001995587022975087\n",
            "#307 Loss: 0.001993773737922311\n",
            "#308 Loss: 0.0019919632468372583\n",
            "#309 Loss: 0.0019901483319699764\n",
            "#310 Loss: 0.001988341799005866\n",
            "#311 Loss: 0.001986537128686905\n",
            "#312 Loss: 0.0019847280345857143\n",
            "#313 Loss: 0.0019829210359603167\n",
            "#314 Loss: 0.0019811142701655626\n",
            "#315 Loss: 0.0019793121609836817\n",
            "#316 Loss: 0.001977505860850215\n",
            "#317 Loss: 0.0019757060799747705\n",
            "#318 Loss: 0.001973905833438039\n",
            "#319 Loss: 0.001972104422748089\n",
            "#320 Loss: 0.0019703025463968515\n",
            "#321 Loss: 0.00196850229986012\n",
            "#322 Loss: 0.0019667022861540318\n",
            "#323 Loss: 0.0019649078603833914\n",
            "#324 Loss: 0.0019631118047982454\n",
            "#325 Loss: 0.0019613157492130995\n",
            "#326 Loss: 0.0019595217891037464\n",
            "#327 Loss: 0.001957728061825037\n",
            "#328 Loss: 0.001955935498699546\n",
            "#329 Loss: 0.0019541403744369745\n",
            "#330 Loss: 0.0019523490918800235\n",
            "#331 Loss: 0.0019505588570609689\n",
            "#332 Loss: 0.0019487693207338452\n",
            "#333 Loss: 0.0019469816470518708\n",
            "#334 Loss: 0.0019451930420473218\n",
            "#335 Loss: 0.0019434094429016113\n",
            "#336 Loss: 0.0019416218856349587\n",
            "#337 Loss: 0.0019398409640416503\n",
            "#338 Loss: 0.0019380515441298485\n",
            "#339 Loss: 0.0019362694583833218\n",
            "#340 Loss: 0.0019344872562214732\n",
            "#341 Loss: 0.0019327091285958886\n",
            "#342 Loss: 0.0019309261115267873\n",
            "#343 Loss: 0.0019291493808850646\n",
            "#344 Loss: 0.0019273716025054455\n",
            "#345 Loss: 0.0019255932420492172\n",
            "#346 Loss: 0.0019238153472542763\n",
            "#347 Loss: 0.0019220393151044846\n",
            "#348 Loss: 0.001920264563523233\n",
            "#349 Loss: 0.0019184882985427976\n",
            "#350 Loss: 0.0019167183199897408\n",
            "#351 Loss: 0.0019149500876665115\n",
            "#352 Loss: 0.0019131754525005817\n",
            "#353 Loss: 0.0019114023307338357\n",
            "#354 Loss: 0.001909633749164641\n",
            "#355 Loss: 0.0019078669138252735\n",
            "#356 Loss: 0.0019060992635786533\n",
            "#357 Loss: 0.001904330332763493\n",
            "#358 Loss: 0.0019025658257305622\n",
            "#359 Loss: 0.0019008032977581024\n",
            "#360 Loss: 0.0018990383250638843\n",
            "#361 Loss: 0.001897278823889792\n",
            "#362 Loss: 0.0018955161795020103\n",
            "#363 Loss: 0.0018937565619125962\n",
            "#364 Loss: 0.0018919961294159293\n",
            "#365 Loss: 0.0018902375595644116\n",
            "#366 Loss: 0.0018884785240516067\n",
            "#367 Loss: 0.0018867248436436057\n",
            "#368 Loss: 0.0018849651096388698\n",
            "#369 Loss: 0.0018832148052752018\n",
            "#370 Loss: 0.0018814600771293044\n",
            "#371 Loss: 0.0018797089578583837\n",
            "#372 Loss: 0.0018779527163133025\n",
            "#373 Loss: 0.0018762071849778295\n",
            "#374 Loss: 0.0018744546687230468\n",
            "#375 Loss: 0.0018727047136053443\n",
            "#376 Loss: 0.0018709594151005149\n",
            "#377 Loss: 0.0018692119047045708\n",
            "#378 Loss: 0.0018674643943086267\n",
            "#379 Loss: 0.0018657228210940957\n",
            "#380 Loss: 0.00186397775541991\n",
            "#381 Loss: 0.0018622358329594135\n",
            "#382 Loss: 0.001860494608990848\n",
            "#383 Loss: 0.0018587574595585465\n",
            "#384 Loss: 0.00185701297596097\n",
            "#385 Loss: 0.001855276059359312\n",
            "#386 Loss: 0.0018535424023866653\n",
            "#387 Loss: 0.0018518072320148349\n",
            "#388 Loss: 0.0018500705482438207\n",
            "#389 Loss: 0.0018483366584405303\n",
            "#390 Loss: 0.0018466018373146653\n",
            "#391 Loss: 0.0018448717892169952\n",
            "#392 Loss: 0.0018431437201797962\n",
            "#393 Loss: 0.0018414094811305404\n",
            "#394 Loss: 0.0018396800151094794\n",
            "#395 Loss: 0.0018379545072093606\n",
            "#396 Loss: 0.0018362313276156783\n",
            "#397 Loss: 0.0018345044227316976\n",
            "#398 Loss: 0.0018327771686017513\n",
            "#399 Loss: 0.001831059344112873\n",
            "#400 Loss: 0.0018293355824425817\n",
            "#401 Loss: 0.001827612635679543\n",
            "#402 Loss: 0.0018258955096825957\n",
            "#403 Loss: 0.0018241772195324302\n",
            "#404 Loss: 0.0018224598607048392\n",
            "#405 Loss: 0.0018207441316917539\n",
            "#406 Loss: 0.0018190304981544614\n",
            "#407 Loss: 0.001817314070649445\n",
            "#408 Loss: 0.0018155999714508653\n",
            "#409 Loss: 0.0018138879677280784\n",
            "#410 Loss: 0.0018121805042028427\n",
            "#411 Loss: 0.0018104695482179523\n",
            "#412 Loss: 0.0018087644129991531\n",
            "#413 Loss: 0.0018070549704134464\n",
            "#414 Loss: 0.0018053501844406128\n",
            "#415 Loss: 0.0018036472611129284\n",
            "#416 Loss: 0.0018019435228779912\n",
            "#417 Loss: 0.0018002376891672611\n",
            "#418 Loss: 0.0017985384911298752\n",
            "#419 Loss: 0.0017968366155400872\n",
            "#420 Loss: 0.0017951411427929997\n",
            "#421 Loss: 0.0017934456700459123\n",
            "#422 Loss: 0.0017917462391778827\n",
            "#423 Loss: 0.0017900507664307952\n",
            "#424 Loss: 0.0017883594846352935\n",
            "#425 Loss: 0.0017866635462269187\n",
            "#426 Loss: 0.0017849741270765662\n",
            "#427 Loss: 0.0017832826124504209\n",
            "#428 Loss: 0.001781596802175045\n",
            "#429 Loss: 0.0017799041233956814\n",
            "#430 Loss: 0.001778221339918673\n",
            "#431 Loss: 0.0017765340162441134\n",
            "#432 Loss: 0.001774849952198565\n",
            "#433 Loss: 0.0017731687985360622\n",
            "#434 Loss: 0.0017714881105348468\n",
            "#435 Loss: 0.001769803580828011\n",
            "#436 Loss: 0.001768125337548554\n",
            "#437 Loss: 0.0017664479091763496\n",
            "#438 Loss: 0.0017647716449573636\n",
            "#439 Loss: 0.001763097126968205\n",
            "#440 Loss: 0.0017614240059629083\n",
            "#441 Loss: 0.001759750652126968\n",
            "#442 Loss: 0.0017580791609361768\n",
            "#443 Loss: 0.0017564097652211785\n",
            "#444 Loss: 0.0017547408351674676\n",
            "#445 Loss: 0.0017530744662508368\n",
            "#446 Loss: 0.0017514076316729188\n",
            "#447 Loss: 0.0017497435910627246\n",
            "#448 Loss: 0.0017480781534686685\n",
            "#449 Loss: 0.0017464151605963707\n",
            "#450 Loss: 0.0017447530990466475\n",
            "#451 Loss: 0.001743094646371901\n",
            "#452 Loss: 0.001741437241435051\n",
            "#453 Loss: 0.0017397800693288445\n",
            "#454 Loss: 0.001738120336085558\n",
            "#455 Loss: 0.0017364677041769028\n",
            "#456 Loss: 0.001734815421514213\n",
            "#457 Loss: 0.0017331624403595924\n",
            "#458 Loss: 0.00173151062335819\n",
            "#459 Loss: 0.00172986404504627\n",
            "#460 Loss: 0.0017282143235206604\n",
            "#461 Loss: 0.0017265662318095565\n",
            "#462 Loss: 0.0017249230295419693\n",
            "#463 Loss: 0.0017232801765203476\n",
            "#464 Loss: 0.0017216355772688985\n",
            "#465 Loss: 0.0017199943540617824\n",
            "#466 Loss: 0.0017183535965159535\n",
            "#467 Loss: 0.0017167160985991359\n",
            "#468 Loss: 0.0017150803469121456\n",
            "#469 Loss: 0.0017134450608864427\n",
            "#470 Loss: 0.0017118091927841306\n",
            "#471 Loss: 0.0017101784469559789\n",
            "#472 Loss: 0.0017085475847125053\n",
            "#473 Loss: 0.001706917304545641\n",
            "#474 Loss: 0.0017052848124876618\n",
            "#475 Loss: 0.001703658141195774\n",
            "#476 Loss: 0.0017020339146256447\n",
            "#477 Loss: 0.0017004079418256879\n",
            "#478 Loss: 0.0016987830167636275\n",
            "#479 Loss: 0.0016971617005765438\n",
            "#480 Loss: 0.0016955441096797585\n",
            "#481 Loss: 0.001693925354629755\n",
            "#482 Loss: 0.0016923053190112114\n",
            "#483 Loss: 0.0016906913369894028\n",
            "#484 Loss: 0.0016890739789232612\n",
            "#485 Loss: 0.001687462325207889\n",
            "#486 Loss: 0.0016858502058312297\n",
            "#487 Loss: 0.0016842391341924667\n",
            "#488 Loss: 0.0016826316714286804\n",
            "#489 Loss: 0.0016810226952657104\n",
            "#490 Loss: 0.0016794184921309352\n",
            "#491 Loss: 0.001677809632383287\n",
            "#492 Loss: 0.0016762047307565808\n",
            "#493 Loss: 0.0016746064648032188\n",
            "#494 Loss: 0.0016730037750676274\n",
            "#495 Loss: 0.0016714035300537944\n",
            "#496 Loss: 0.0016698086401447654\n",
            "#497 Loss: 0.001668210607022047\n",
            "#498 Loss: 0.0016666156006976962\n",
            "#499 Loss: 0.0016650207107886672\n",
            "#500 Loss: 0.0016634290805086493\n",
            "#501 Loss: 0.0016618430381640792\n",
            "#502 Loss: 0.0016602487303316593\n",
            "#503 Loss: 0.0016586597776040435\n",
            "#504 Loss: 0.001657075365073979\n",
            "#505 Loss: 0.0016554895555600524\n",
            "#506 Loss: 0.0016539081698283553\n",
            "#507 Loss: 0.0016523245722055435\n",
            "#508 Loss: 0.001650746911764145\n",
            "#509 Loss: 0.0016491696005687118\n",
            "#510 Loss: 0.0016475915908813477\n",
            "#511 Loss: 0.0016460154438391328\n",
            "#512 Loss: 0.0016444410430267453\n",
            "#513 Loss: 0.0016428680391982198\n",
            "#514 Loss: 0.001641298527829349\n",
            "#515 Loss: 0.0016397255240008235\n",
            "#516 Loss: 0.0016381606692448258\n",
            "#517 Loss: 0.0016365902265533805\n",
            "#518 Loss: 0.0016350258374586701\n",
            "#519 Loss: 0.0016334602842107415\n",
            "#520 Loss: 0.001631900668144226\n",
            "#521 Loss: 0.001630338723771274\n",
            "#522 Loss: 0.0016287820180878043\n",
            "#523 Loss: 0.001627221587114036\n",
            "#524 Loss: 0.001625663717277348\n",
            "#525 Loss: 0.0016241123666986823\n",
            "#526 Loss: 0.0016225577564910054\n",
            "#527 Loss: 0.001621007570065558\n",
            "#528 Loss: 0.0016194552881643176\n",
            "#529 Loss: 0.0016179048689082265\n",
            "#530 Loss: 0.0016163574764505029\n",
            "#531 Loss: 0.0016148117138072848\n",
            "#532 Loss: 0.001613267115317285\n",
            "#533 Loss: 0.0016117239138111472\n",
            "#534 Loss: 0.0016101818764582276\n",
            "#535 Loss: 0.0016086449613794684\n",
            "#536 Loss: 0.0016071051359176636\n",
            "#537 Loss: 0.0016055713640525937\n",
            "#538 Loss: 0.0016040368936955929\n",
            "#539 Loss: 0.0016025025397539139\n",
            "#540 Loss: 0.001600967370904982\n",
            "#541 Loss: 0.0015994386048987508\n",
            "#542 Loss: 0.0015979106537997723\n",
            "#543 Loss: 0.00159637990873307\n",
            "#544 Loss: 0.0015948564978316426\n",
            "#545 Loss: 0.0015933349495753646\n",
            "#546 Loss: 0.0015918094431981444\n",
            "#547 Loss: 0.0015902891755104065\n",
            "#548 Loss: 0.0015887669287621975\n",
            "#549 Loss: 0.0015872515505179763\n",
            "#550 Loss: 0.0015857337275519967\n",
            "#551 Loss: 0.0015842214925214648\n",
            "#552 Loss: 0.0015827059978619218\n",
            "#553 Loss: 0.0015811929479241371\n",
            "#554 Loss: 0.0015796854859218001\n",
            "#555 Loss: 0.0015781769761815667\n",
            "#556 Loss: 0.001576667302288115\n",
            "#557 Loss: 0.001575159840285778\n",
            "#558 Loss: 0.0015736608766019344\n",
            "#559 Loss: 0.0015721586532890797\n",
            "#560 Loss: 0.001570656429976225\n",
            "#561 Loss: 0.0015691532753407955\n",
            "#562 Loss: 0.0015676560578867793\n",
            "#563 Loss: 0.001566159538924694\n",
            "#564 Loss: 0.001564665581099689\n",
            "#565 Loss: 0.0015631740679964423\n",
            "#566 Loss: 0.0015616813907399774\n",
            "#567 Loss: 0.0015601924387738109\n",
            "#568 Loss: 0.00155870511662215\n",
            "#569 Loss: 0.0015572156989946961\n",
            "#570 Loss: 0.0015557329170405865\n",
            "#571 Loss: 0.0015542510664090514\n",
            "#572 Loss: 0.0015527693321928382\n",
            "#573 Loss: 0.0015512852696701884\n",
            "#574 Loss: 0.0015498064458370209\n",
            "#575 Loss: 0.0015483311144635081\n",
            "#576 Loss: 0.0015468569472432137\n",
            "#577 Loss: 0.001545384875498712\n",
            "#578 Loss: 0.001543909776955843\n",
            "#579 Loss: 0.0015424401499330997\n",
            "#580 Loss: 0.001540972269140184\n",
            "#581 Loss: 0.0015395022928714752\n",
            "#582 Loss: 0.0015380365075543523\n",
            "#583 Loss: 0.0015365747967734933\n",
            "#584 Loss: 0.0015351111069321632\n",
            "#585 Loss: 0.0015336481155827641\n",
            "#586 Loss: 0.001532187801785767\n",
            "#587 Loss: 0.0015307329595088959\n",
            "#588 Loss: 0.0015292740426957607\n",
            "#589 Loss: 0.001527822925709188\n",
            "#590 Loss: 0.0015263687819242477\n",
            "#591 Loss: 0.001524918363429606\n",
            "#592 Loss: 0.001523466664366424\n",
            "#593 Loss: 0.001522018457762897\n",
            "#594 Loss: 0.0015205717645585537\n",
            "#595 Loss: 0.0015191249549388885\n",
            "#596 Loss: 0.0015176850138232112\n",
            "#597 Loss: 0.0015162414638325572\n",
            "#598 Loss: 0.0015147984959185123\n",
            "#599 Loss: 0.0015133637934923172\n",
            "#600 Loss: 0.0015119254821911454\n",
            "#601 Loss: 0.001510488917119801\n",
            "#602 Loss: 0.0015090591041371226\n",
            "#603 Loss: 0.001507625333033502\n",
            "#604 Loss: 0.0015061922604218125\n",
            "#605 Loss: 0.0015047634951770306\n",
            "#606 Loss: 0.0015033347299322486\n",
            "#607 Loss: 0.001501912367530167\n",
            "#608 Loss: 0.0015004867454990745\n",
            "#609 Loss: 0.0014990628696978092\n",
            "#610 Loss: 0.0014976469101384282\n",
            "#611 Loss: 0.0014962268760427833\n",
            "#612 Loss: 0.0014948075404390693\n",
            "#613 Loss: 0.0014933926286175847\n",
            "#614 Loss: 0.00149197643622756\n",
            "#615 Loss: 0.0014905632706359029\n",
            "#616 Loss: 0.0014891563914716244\n",
            "#617 Loss: 0.0014877468347549438\n",
            "#618 Loss: 0.0014863363467156887\n",
            "#619 Loss: 0.0014849317958578467\n",
            "#620 Loss: 0.0014835312031209469\n",
            "#621 Loss: 0.0014821254881098866\n",
            "#622 Loss: 0.0014807251282036304\n",
            "#623 Loss: 0.001479321508668363\n",
            "#624 Loss: 0.0014779269695281982\n",
            "#625 Loss: 0.001476528705097735\n",
            "#626 Loss: 0.0014751376584172249\n",
            "#627 Loss: 0.0014737447490915656\n",
            "#628 Loss: 0.0014723512576892972\n",
            "#629 Loss: 0.0014709612587466836\n",
            "#630 Loss: 0.0014695724239572883\n",
            "#631 Loss: 0.0014681891771033406\n",
            "#632 Loss: 0.0014668031362816691\n",
            "#633 Loss: 0.0014654189581051469\n",
            "#634 Loss: 0.0014640368754044175\n",
            "#635 Loss: 0.0014626552583649755\n",
            "#636 Loss: 0.0014612808590754867\n",
            "#637 Loss: 0.0014599044807255268\n",
            "#638 Loss: 0.001458528102375567\n",
            "#639 Loss: 0.001457152422517538\n",
            "#640 Loss: 0.0014557857066392899\n",
            "#641 Loss: 0.0014544157311320305\n",
            "#642 Loss: 0.0014530461048707366\n",
            "#643 Loss: 0.0014516771771013737\n",
            "#644 Loss: 0.0014503110432997346\n",
            "#645 Loss: 0.001448950613848865\n",
            "#646 Loss: 0.0014475872740149498\n",
            "#647 Loss: 0.001446226960979402\n",
            "#648 Loss: 0.0014448714209720492\n",
            "#649 Loss: 0.0014435147168114781\n",
            "#650 Loss: 0.0014421582454815507\n",
            "#651 Loss: 0.001440805965103209\n",
            "#652 Loss: 0.0014394531026482582\n",
            "#653 Loss: 0.0014381023356691003\n",
            "#654 Loss: 0.0014367556432262063\n",
            "#655 Loss: 0.0014354068553075194\n",
            "#656 Loss: 0.0014340636553242803\n",
            "#657 Loss: 0.0014327174285426736\n",
            "#658 Loss: 0.0014313742285594344\n",
            "#659 Loss: 0.0014300355687737465\n",
            "#660 Loss: 0.0014286957448348403\n",
            "#661 Loss: 0.0014273604610934854\n",
            "#662 Loss: 0.0014260224997997284\n",
            "#663 Loss: 0.0014246910577639937\n",
            "#664 Loss: 0.001423357636667788\n",
            "#665 Loss: 0.0014220275916159153\n",
            "#666 Loss: 0.001420702668838203\n",
            "#667 Loss: 0.0014193701790645719\n",
            "#668 Loss: 0.0014180495636537671\n",
            "#669 Loss: 0.0014167199842631817\n",
            "#670 Loss: 0.0014154015807434916\n",
            "#671 Loss: 0.0014140818966552615\n",
            "#672 Loss: 0.0014127576723694801\n",
            "#673 Loss: 0.001411444041877985\n",
            "#674 Loss: 0.0014101288979873061\n",
            "#675 Loss: 0.0014088164316490293\n",
            "#676 Loss: 0.0014074989594519138\n",
            "#677 Loss: 0.0014061908004805446\n",
            "#678 Loss: 0.0014048811281099916\n",
            "#679 Loss: 0.0014035747153684497\n",
            "#680 Loss: 0.0014022692339494824\n",
            "#681 Loss: 0.0014009643346071243\n",
            "#682 Loss: 0.001399661530740559\n",
            "#683 Loss: 0.0013983588432893157\n",
            "#684 Loss: 0.0013970611616969109\n",
            "#685 Loss: 0.0013957653427496552\n",
            "#686 Loss: 0.0013944689417257905\n",
            "#687 Loss: 0.0013931746361777186\n",
            "#688 Loss: 0.0013918824261054397\n",
            "#689 Loss: 0.0013905904488638043\n",
            "#690 Loss: 0.0013893001014366746\n",
            "#691 Loss: 0.0013880105689167976\n",
            "#692 Loss: 0.001386726158671081\n",
            "#693 Loss: 0.001385440700687468\n",
            "#694 Loss: 0.0013841580366715789\n",
            "#695 Loss: 0.0013828800292685628\n",
            "#696 Loss: 0.001381598529405892\n",
            "#697 Loss: 0.001380317728035152\n",
            "#698 Loss: 0.001379043678753078\n",
            "#699 Loss: 0.0013777666026726365\n",
            "#700 Loss: 0.001376497675664723\n",
            "#701 Loss: 0.0013752266531810164\n",
            "#702 Loss: 0.0013739537680521607\n",
            "#703 Loss: 0.0013726878678426147\n",
            "#704 Loss: 0.0013714199885725975\n",
            "#705 Loss: 0.0013701552525162697\n",
            "#706 Loss: 0.0013688941253349185\n",
            "#707 Loss: 0.0013676321832463145\n",
            "#708 Loss: 0.0013663694262504578\n",
            "#709 Loss: 0.001365110743790865\n",
            "#710 Loss: 0.0013638563686981797\n",
            "#711 Loss: 0.0013625995488837361\n",
            "#712 Loss: 0.001361347734928131\n",
            "#713 Loss: 0.0013600954553112388\n",
            "#714 Loss: 0.0013588439906015992\n",
            "#715 Loss: 0.0013575964840129018\n",
            "#716 Loss: 0.0013563527027145028\n",
            "#717 Loss: 0.0013551049632951617\n",
            "#718 Loss: 0.0013538626953959465\n",
            "#719 Loss: 0.0013526190305128694\n",
            "#720 Loss: 0.0013513806043192744\n",
            "#721 Loss: 0.001350140548311174\n",
            "#722 Loss: 0.0013489011907950044\n",
            "#723 Loss: 0.0013476694002747536\n",
            "#724 Loss: 0.001346432138234377\n",
            "#725 Loss: 0.0013452018611133099\n",
            "#726 Loss: 0.001343972864560783\n",
            "#727 Loss: 0.0013427400263026357\n",
            "#728 Loss: 0.0013415137073025107\n",
            "#729 Loss: 0.001340289949439466\n",
            "#730 Loss: 0.0013390617677941918\n",
            "#731 Loss: 0.0013378405710682273\n",
            "#732 Loss: 0.0013366201892495155\n",
            "#733 Loss: 0.001335402368567884\n",
            "#734 Loss: 0.0013341843150556087\n",
            "#735 Loss: 0.0013329683570191264\n",
            "#736 Loss: 0.001331752515397966\n",
            "#737 Loss: 0.001330538303591311\n",
            "#738 Loss: 0.0013293320080265403\n",
            "#739 Loss: 0.0013281217543408275\n",
            "#740 Loss: 0.0013269110349938273\n",
            "#741 Loss: 0.0013257036916911602\n",
            "#742 Loss: 0.0013245005393400788\n",
            "#743 Loss: 0.0013232966884970665\n",
            "#744 Loss: 0.0013220972614362836\n",
            "#745 Loss: 0.0013208974851295352\n",
            "#746 Loss: 0.0013196965446695685\n",
            "#747 Loss: 0.0013185027055442333\n",
            "#748 Loss: 0.0013173079350963235\n",
            "#749 Loss: 0.0013161107199266553\n",
            "#750 Loss: 0.0013149190926924348\n",
            "#751 Loss: 0.0013137311907485127\n",
            "#752 Loss: 0.0013125414261594415\n",
            "#753 Loss: 0.0013113523600623012\n",
            "#754 Loss: 0.0013101681834086776\n",
            "#755 Loss: 0.0013089863350614905\n",
            "#756 Loss: 0.0013078040210530162\n",
            "#757 Loss: 0.0013066214742138982\n",
            "#758 Loss: 0.0013054418377578259\n",
            "#759 Loss: 0.0013042661594226956\n",
            "#760 Loss: 0.0013030879199504852\n",
            "#761 Loss: 0.0013019158504903316\n",
            "#762 Loss: 0.0013007408706471324\n",
            "#763 Loss: 0.001299571362324059\n",
            "#764 Loss: 0.0012984020868316293\n",
            "#765 Loss: 0.001297231181524694\n",
            "#766 Loss: 0.0012960649328306317\n",
            "#767 Loss: 0.001294901012443006\n",
            "#768 Loss: 0.0012937383726239204\n",
            "#769 Loss: 0.0012925768969580531\n",
            "#770 Loss: 0.001291416003368795\n",
            "#771 Loss: 0.001290257670916617\n",
            "#772 Loss: 0.001289105392061174\n",
            "#773 Loss: 0.0012879460118710995\n",
            "#774 Loss: 0.0012867918703705072\n",
            "#775 Loss: 0.0012856435496360064\n",
            "#776 Loss: 0.0012844912707805634\n",
            "#777 Loss: 0.0012833431828767061\n",
            "#778 Loss: 0.0012821928830817342\n",
            "#779 Loss: 0.0012810471234843135\n",
            "#780 Loss: 0.0012799050891771913\n",
            "#781 Loss: 0.0012787638697773218\n",
            "#782 Loss: 0.001277622883208096\n",
            "#783 Loss: 0.0012764801504090428\n",
            "#784 Loss: 0.0012753453338518739\n",
            "#785 Loss: 0.0012742073740810156\n",
            "#786 Loss: 0.0012730733724310994\n",
            "#787 Loss: 0.0012719385558739305\n",
            "#788 Loss: 0.001270806067623198\n",
            "#789 Loss: 0.0012696783524006605\n",
            "#790 Loss: 0.0012685471447184682\n",
            "#791 Loss: 0.0012674216413870454\n",
            "#792 Loss: 0.0012662946246564388\n",
            "#793 Loss: 0.0012651719152927399\n",
            "#794 Loss: 0.0012640510685741901\n",
            "#795 Loss: 0.0012629316188395023\n",
            "#796 Loss: 0.0012618115870282054\n",
            "#797 Loss: 0.0012606901582330465\n",
            "#798 Loss: 0.0012595781590789557\n",
            "#799 Loss: 0.0012584637152031064\n",
            "#800 Loss: 0.001257348689250648\n",
            "#801 Loss: 0.0012562359916046262\n",
            "#802 Loss: 0.0012551291147246957\n",
            "#803 Loss: 0.0012540187453851104\n",
            "#804 Loss: 0.0012529116356745362\n",
            "#805 Loss: 0.0012518089497461915\n",
            "#806 Loss: 0.0012507044011726975\n",
            "#807 Loss: 0.0012495993869379163\n",
            "#808 Loss: 0.001248499727807939\n",
            "#809 Loss: 0.0012474023969843984\n",
            "#810 Loss: 0.0012463027378544211\n",
            "#811 Loss: 0.001245207735337317\n",
            "#812 Loss: 0.0012441150611266494\n",
            "#813 Loss: 0.0012430191272869706\n",
            "#814 Loss: 0.0012419301783666015\n",
            "#815 Loss: 0.0012408389011397958\n",
            "#816 Loss: 0.0012397520476952195\n",
            "#817 Loss: 0.0012386639136821032\n",
            "#818 Loss: 0.0012375802034512162\n",
            "#819 Loss: 0.0012364945141598582\n",
            "#820 Loss: 0.001235413015820086\n",
            "#821 Loss: 0.0012343324488028884\n",
            "#822 Loss: 0.0012332542100921273\n",
            "#823 Loss: 0.0012321773683652282\n",
            "#824 Loss: 0.0012310987804085016\n",
            "#825 Loss: 0.001230022986419499\n",
            "#826 Loss: 0.0012289519654586911\n",
            "#827 Loss: 0.001227879780344665\n",
            "#828 Loss: 0.001226810272783041\n",
            "#829 Loss: 0.0012257433263584971\n",
            "#830 Loss: 0.001224674517288804\n",
            "#831 Loss: 0.001223609084263444\n",
            "#832 Loss: 0.0012225480750203133\n",
            "#833 Loss: 0.0012214829912409186\n",
            "#834 Loss: 0.0012204233789816499\n",
            "#835 Loss: 0.0012193593429401517\n",
            "#836 Loss: 0.0012183054350316525\n",
            "#837 Loss: 0.0012172487331554294\n",
            "#838 Loss: 0.001216192147694528\n",
            "#839 Loss: 0.0012151362607255578\n",
            "#840 Loss: 0.0012140875915065408\n",
            "#841 Loss: 0.001213038805872202\n",
            "#842 Loss: 0.0012119857128709555\n",
            "#843 Loss: 0.0012109408853575587\n",
            "#844 Loss: 0.001209895359352231\n",
            "#845 Loss: 0.0012088518124073744\n",
            "#846 Loss: 0.0012078076833859086\n",
            "#847 Loss: 0.0012067670468240976\n",
            "#848 Loss: 0.0012057252461090684\n",
            "#849 Loss: 0.0012046868214383721\n",
            "#850 Loss: 0.0012036483967676759\n",
            "#851 Loss: 0.0012026128824800253\n",
            "#852 Loss: 0.0012015796964988112\n",
            "#853 Loss: 0.00120054732542485\n",
            "#854 Loss: 0.001199516816996038\n",
            "#855 Loss: 0.0011984879383817315\n",
            "#856 Loss: 0.0011974578956142068\n",
            "#857 Loss: 0.001196429948322475\n",
            "#858 Loss: 0.0011954045621678233\n",
            "#859 Loss: 0.0011943811550736427\n",
            "#860 Loss: 0.0011933611240237951\n",
            "#861 Loss: 0.0011923382990062237\n",
            "#862 Loss: 0.0011913212947547436\n",
            "#863 Loss: 0.0011903020786121488\n",
            "#864 Loss: 0.0011892877519130707\n",
            "#865 Loss: 0.001188271795399487\n",
            "#866 Loss: 0.001187254674732685\n",
            "#867 Loss: 0.0011862438404932618\n",
            "#868 Loss: 0.001185234053991735\n",
            "#869 Loss: 0.0011842251988127828\n",
            "#870 Loss: 0.0011832149466499686\n",
            "#871 Loss: 0.0011822126107290387\n",
            "#872 Loss: 0.0011812077136710286\n",
            "#873 Loss: 0.0011801993241533637\n",
            "#874 Loss: 0.0011792018776759505\n",
            "#875 Loss: 0.001178200007416308\n",
            "#876 Loss: 0.0011772046564146876\n",
            "#877 Loss: 0.0011762030189856887\n",
            "#878 Loss: 0.0011752075515687466\n",
            "#879 Loss: 0.0011742120841518044\n",
            "#880 Loss: 0.0011732220882549882\n",
            "#881 Loss: 0.0011722309282049537\n",
            "#882 Loss: 0.0011712388368323445\n",
            "#883 Loss: 0.0011702502379193902\n",
            "#884 Loss: 0.0011692600091919303\n",
            "#885 Loss: 0.0011682756012305617\n",
            "#886 Loss: 0.0011672890977934003\n",
            "#887 Loss: 0.001166307250969112\n",
            "#888 Loss: 0.0011653255205601454\n",
            "#889 Loss: 0.0011643444886431098\n",
            "#890 Loss: 0.001163365552201867\n",
            "#891 Loss: 0.0011623915052041411\n",
            "#892 Loss: 0.001161414198577404\n",
            "#893 Loss: 0.0011604364262893796\n",
            "#894 Loss: 0.0011594644747674465\n",
            "#895 Loss: 0.0011584945023059845\n",
            "#896 Loss: 0.0011575211538001895\n",
            "#897 Loss: 0.0011565539753064513\n",
            "#898 Loss: 0.0011555873788893223\n",
            "#899 Loss: 0.0011546186869964004\n",
            "#900 Loss: 0.001153654302470386\n",
            "#901 Loss: 0.0011526905000209808\n",
            "#902 Loss: 0.0011517299572005868\n",
            "#903 Loss: 0.0011507690651342273\n",
            "#904 Loss: 0.0011498106177896261\n",
            "#905 Loss: 0.001148852868936956\n",
            "#906 Loss: 0.001147894305177033\n",
            "#907 Loss: 0.0011469393502920866\n",
            "#908 Loss: 0.001145987887866795\n",
            "#909 Loss: 0.0011450346792116761\n",
            "#910 Loss: 0.0011440839152783155\n",
            "#911 Loss: 0.0011431375751271844\n",
            "#912 Loss: 0.001142187975347042\n",
            "#913 Loss: 0.001141239539720118\n",
            "#914 Loss: 0.0011402955278754234\n",
            "#915 Loss: 0.001139350119046867\n",
            "#916 Loss: 0.0011384087847545743\n",
            "#917 Loss: 0.0011374671012163162\n",
            "#918 Loss: 0.0011365276295691729\n",
            "#919 Loss: 0.0011355893220752478\n",
            "#920 Loss: 0.00113465276081115\n",
            "#921 Loss: 0.0011337172472849488\n",
            "#922 Loss: 0.001132781500928104\n",
            "#923 Loss: 0.0011318470351397991\n",
            "#924 Loss: 0.001130917458795011\n",
            "#925 Loss: 0.0011299899779260159\n",
            "#926 Loss: 0.0011290623806416988\n",
            "#927 Loss: 0.0011281287297606468\n",
            "#928 Loss: 0.0011272038100287318\n",
            "#929 Loss: 0.0011262844782322645\n",
            "#930 Loss: 0.0011253576958552003\n",
            "#931 Loss: 0.0011244365014135838\n",
            "#932 Loss: 0.0011235139099881053\n",
            "#933 Loss: 0.0011225960915908217\n",
            "#934 Loss: 0.0011216768762096763\n",
            "#935 Loss: 0.001120760221965611\n",
            "#936 Loss: 0.0011198441497981548\n",
            "#937 Loss: 0.0011189322685822845\n",
            "#938 Loss: 0.0011180188739672303\n",
            "#939 Loss: 0.0011171059450134635\n",
            "#940 Loss: 0.0011161986039951444\n",
            "#941 Loss: 0.0011152904480695724\n",
            "#942 Loss: 0.0011143857846036553\n",
            "#943 Loss: 0.001113475882448256\n",
            "#944 Loss: 0.0011125729652121663\n",
            "#945 Loss: 0.0011116712121292949\n",
            "#946 Loss: 0.0011107687605544925\n",
            "#947 Loss: 0.001109867007471621\n",
            "#948 Loss: 0.001108968281187117\n",
            "#949 Loss: 0.001108068390749395\n",
            "#950 Loss: 0.0011071752524003386\n",
            "#951 Loss: 0.001106277690269053\n",
            "#952 Loss: 0.001105387113057077\n",
            "#953 Loss: 0.0011044921120628715\n",
            "#954 Loss: 0.0011035996722057462\n",
            "#955 Loss: 0.0011027106083929539\n",
            "#956 Loss: 0.0011018224759027362\n",
            "#957 Loss: 0.0011009334120899439\n",
            "#958 Loss: 0.0011000465601682663\n",
            "#959 Loss: 0.001099162152968347\n",
            "#960 Loss: 0.0010982798412442207\n",
            "#961 Loss: 0.0010974007891491055\n",
            "#962 Loss: 0.0010965156834572554\n",
            "#963 Loss: 0.0010956382611766458\n",
            "#964 Loss: 0.0010947593254968524\n",
            "#965 Loss: 0.0010938815539702773\n",
            "#966 Loss: 0.0010930075077340007\n",
            "#967 Loss: 0.0010921309003606439\n",
            "#968 Loss: 0.001091259764507413\n",
            "#969 Loss: 0.0010903883958235383\n",
            "#970 Loss: 0.0010895199375227094\n",
            "#971 Loss: 0.0010886475211009383\n",
            "#972 Loss: 0.0010877805761992931\n",
            "#973 Loss: 0.0010869139805436134\n",
            "#974 Loss: 0.0010860481997951865\n",
            "#975 Loss: 0.0010851833503693342\n",
            "#976 Loss: 0.0010843193158507347\n",
            "#977 Loss: 0.0010834619170054793\n",
            "#978 Loss: 0.0010825986973941326\n",
            "#979 Loss: 0.0010817417642101645\n",
            "#980 Loss: 0.0010808827355504036\n",
            "#981 Loss: 0.001080027548596263\n",
            "#982 Loss: 0.0010791714303195477\n",
            "#983 Loss: 0.0010783178731799126\n",
            "#984 Loss: 0.0010774648981168866\n",
            "#985 Loss: 0.0010766144841909409\n",
            "#986 Loss: 0.0010757659329101443\n",
            "#987 Loss: 0.0010749157518148422\n",
            "#988 Loss: 0.001074068364687264\n",
            "#989 Loss: 0.0010732249356806278\n",
            "#990 Loss: 0.0010723754530772567\n",
            "#991 Loss: 0.0010715331882238388\n",
            "#992 Loss: 0.00107069022487849\n",
            "#993 Loss: 0.0010698485421016812\n",
            "#994 Loss: 0.001069011283107102\n",
            "#995 Loss: 0.0010681698331609368\n",
            "#996 Loss: 0.0010673367651179433\n",
            "#997 Loss: 0.0010664970614016056\n",
            "#998 Loss: 0.0010656629456207156\n",
            "#999 Loss: 0.0010648294119164348\n",
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([0.9404])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}